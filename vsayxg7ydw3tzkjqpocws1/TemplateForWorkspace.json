{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "vsayxg7ydw3tzkjqpocws1"
		},
		"CosmosDBLink_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'CosmosDBLink'"
		},
		"keyVaultLinkedservice_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "@{concat('https://',linkedService().keyVaultName,'.vault.azure.net/')}"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/CosmosDBLink')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "CosmosDb",
				"typeProperties": {
					"connectionString": "[parameters('CosmosDBLink_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/keyVaultLinkedservice')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"keyVaultName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('keyVaultLinkedservice_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0,
							"cleanup": true
						},
						"pipelineExternalComputeScaleProperties": {
							"timeToLive": 60
						}
					}
				},
				"managedVirtualNetwork": {
					"type": "ManagedVirtualNetworkReference",
					"referenceName": "default"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/1-SalesForecastingWithAML')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "ws1sparkpool1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ca9f62b4-724e-47d5-b7de-bef91a854107"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/75d3d9a6-7fad-46e1-bea2-4d2140013c78/resourceGroups/vsasynapsepoc/providers/Microsoft.Synapse/workspaces/vsayxg7ydw3tzkjqpocws1/bigDataPools/ws1sparkpool1",
						"name": "ws1sparkpool1",
						"type": "Spark",
						"endpoint": "https://vsayxg7ydw3tzkjqpocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ws1sparkpool1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"# Leverage Power of Azure Synapse Link for Cosmos DB and Spark SQL\n",
							"With Synapse Link, you can now directly connect to your Azure Cosmos DB containers from Azure Synapse Analytics and access the analytical store with no separate connectors. This notebook scenario is to \n",
							"- Ingest data into Cosmos DB containers using Azure Synapse Spark\n",
							"- Setup Spark tables\n",
							"- Join & aggregate operational data across Cosmos DB containers\n",
							"- Perform Near real-time Sales Forecasting using Azure Automated Machine Learning using Synapse Link for Cosmos DB"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<img src=\"https://synapse1poc.blob.core.windows.net/cosmolnksynp/cosmolnksynp/synapse-cosmosdb.png\" alt=\"Surface Device\" width=\"75%\"/>\r\n",
							"\r\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Replace all \"AML\" placeholders with your own information,Replace subscription_id , resource_group with values before execution\r\n",
							"<img src=\"https://synapse1poc.blob.core.windows.net/cosmolnksynp/cosmolnksynp/SF_Ws_Variables.gif\" alt=\"Surface Device\" width=\"75%\"/>\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"AMLWsname='vsayxg7ydw3tzkjqpocws1'\r\n",
							"AMLWssubscription_id='75d3d9a6-7fad-46e1-bea2-4d2140013c78'\r\n",
							"AMLWsresource_group='vsasynapsepoc'"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Data Ingestion into Cosmos DB using Synapse Cosmos DB Link"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"path = 'wasbs://cosmolnksynp@synapse1poc.blob.core.windows.net/cosmolnksynp/Products.csv'\r\n",
							"dfProducts = spark.read.csv(path, header=True, inferSchema=True)\r\n",
							"dfProducts.write\\\r\n",
							"    .format(\"cosmos.oltp\")\\\r\n",
							"    .option(\"spark.synapse.linkedService\", \"CosmosDBLink\")\\\r\n",
							"    .option(\"spark.cosmos.container\", \"Products\")\\\r\n",
							"    .option(\"spark.cosmos.write.upsertEnabled\", \"true\")\\\r\n",
							"    .mode('append')\\\r\n",
							"    .save()\r\n",
							"\r\n",
							"path = 'wasbs://cosmolnksynp@synapse1poc.blob.core.windows.net/cosmolnksynp/RetailSales.csv'\r\n",
							"dfRetailSales = spark.read.csv(path, header=True, inferSchema=True)\r\n",
							"dfRetailSales.write\\\r\n",
							"    .format(\"cosmos.oltp\")\\\r\n",
							"    .option(\"spark.synapse.linkedService\", \"CosmosDBLink\")\\\r\n",
							"    .option(\"spark.cosmos.container\", \"RetailSales\")\\\r\n",
							"    .option(\"spark.cosmos.write.upsertEnabled\", \"true\")\\\r\n",
							"    .mode('append')\\\r\n",
							"    .save()\r\n",
							"\r\n",
							"path = 'wasbs://cosmolnksynp@synapse1poc.blob.core.windows.net/cosmolnksynp/StoreDemoGraphics.csv'\r\n",
							"dfStoreDemoGraphics = spark.read.csv(path, header=True, inferSchema=True)\r\n",
							"dfStoreDemoGraphics.write\\\r\n",
							"    .format(\"cosmos.oltp\")\\\r\n",
							"    .option(\"spark.synapse.linkedService\", \"CosmosDBLink\")\\\r\n",
							"    .option(\"spark.cosmos.container\", \"StoreDemoGraphics\")\\\r\n",
							"    .option(\"spark.cosmos.write.upsertEnabled\", \"true\")\\\r\n",
							"    .mode('append')\\\r\n",
							"    .save()"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create Spark tables pointing to the Azure Cosmos DB Analytical Store collections using Azure Synapse Link"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"create database if not exists RetailSalesDB;\n",
							"\n",
							"create table if not exists RetailSalesDB.RetailSales using cosmos.olap options (\n",
							"    spark.synapse.linkedService 'CosmosDBLink',\n",
							"    spark.cosmos.preferredRegions 'South Central US',  --Optional, Update If Required\n",
							"    spark.cosmos.container 'RetailSales'\n",
							");\n",
							"\n",
							"create table if not exists RetailSalesDB.StoreDemographics using cosmos.olap options (\n",
							"    spark.synapse.linkedService 'CosmosDBLink',\n",
							"    spark.cosmos.preferredRegions 'South Central US', --Optional, Update If Required\n",
							"    spark.cosmos.container 'StoreDemoGraphics'\n",
							");\n",
							"create table if not exists RetailSalesDB.Product using cosmos.olap options (\n",
							"    spark.synapse.linkedService 'CosmosDBLink',\n",
							"    spark.cosmos.preferredRegions 'South Central US', --Optional, Update If Required\n",
							"    spark.cosmos.container 'Products'\n",
							");"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Perform Joins across collections, apply filters and aggregations using Spark SQL"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"data = spark.sql(\"select a.storeId \\\n",
							"                       , b.productCode \\\n",
							"                       , b.wholeSaleCost \\\n",
							"                       , b.basePrice \\\n",
							"                       , c.ratioAge60 \\\n",
							"                       , c.collegeRatio \\\n",
							"                       , c.income \\\n",
							"                       , c.highIncome150Ratio \\\n",
							"                       , c.largeHH \\\n",
							"                       , c.minoritiesRatio \\\n",
							"                       , c.more1FullTimeEmployeeRatio \\\n",
							"                       , c.distanceNearestWarehouse \\\n",
							"                       , c.salesNearestWarehousesRatio \\\n",
							"                       , c.avgDistanceNearest5Supermarkets \\\n",
							"                       , c.salesNearest5StoresRatio \\\n",
							"                       , a.quantity \\\n",
							"                       , a.logQuantity \\\n",
							"                       , a.advertising \\\n",
							"                       , a.price \\\n",
							"                       , a.weekStarting \\\n",
							"                 from RetailSalesDB.retailsales a \\\n",
							"                 left join RetailSalesDB.product b \\\n",
							"                 on a.productcode = b.productcode \\\n",
							"                 left join RetailSalesDB.storedemographics c \\\n",
							"                 on a.storeId = c.storeId \\\n",
							"                 order by a.weekStarting, a.storeId, b.productCode\")\n",
							"\n",
							"display(data)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"data.write.parquet('abfss://dlsvsapocfs1@vsayxg7ydw3tzkjqpoc.dfs.core.windows.net/demodata/demo_df', mode='overwrite')"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"SHOW TABLES"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"## Predictive Analytics\n",
							"Leverage power of Azure Machine Learning's AutoML to build a Forecasting Model Predictive analytics can help us to study and discover the factors that determine the number of sales that a retail store will have in the future.This notebook scenario is [Microsoft Surface](https://www.microsoft.com/en-us/surface) sales forecasting, with artificially created data. The business challenge is a distributor that wants to predict how many units are necessary in the local warehouse to supply the stores in the area.\n",
							"We will use Quantitative Models to forecast future data as a function of past data. They are appropriate to use when past numerical data is available and when it is reasonable to assume that some of the patterns in the data are expected to continue into the future. These methods are usually applied to short or intermediate range decisions. For more information, click [here](https://en.wikipedia.org/wiki/Forecasting).\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from azureml.core import Workspace\r\n",
							"\r\n",
							"ws = Workspace.create(name=AMLWsname,\r\n",
							"               subscription_id=AMLWssubscription_id,\r\n",
							"               resource_group=AMLWsresource_group,\r\n",
							"               create_resource_group=True,\r\n",
							"               location='South Central US' ## Optional, Update If Required\r\n",
							"               )"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"import azureml.core\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"import logging\n",
							"from azureml.core.workspace import Workspace\n",
							"from azureml.core import Workspace\n",
							"from azureml.core.experiment import Experiment\n",
							"from azureml.train.automl import AutoMLConfig\n",
							"import os\n",
							"subscription_id = ws.subscription_id\n",
							"resource_group = ws.resource_group\n",
							"workspace_name = ws._workspace_name\n",
							"workspace_region = 'South Central US' ## Optional, Update If Required\n",
							"ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
							"ws.write_config()\n",
							"    \n",
							"experiment_name = 'automl-surfaceforecasting'\n",
							"experiment = Experiment(ws, experiment_name)\n",
							"output = {}\n",
							"output['Subscription ID'] = ws.subscription_id\n",
							"output['Workspace'] = ws.name\n",
							"output['SKU'] = ws.sku\n",
							"output['Resource Group'] = ws.resource_group\n",
							"output['Location'] = ws.location\n",
							"output['Run History Name'] = experiment_name\n",
							"pd.set_option('display.max_colwidth', -1)\n",
							"outputDf = pd.DataFrame(data = output, index = [''])\n",
							"outputDf.T"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Data Preparation - Feature engineering, Splitting train & test datasets\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Initial variables\n",
							"time_column_name = 'weekStarting'\n",
							"grain_column_names = ['storeId', 'productCode']\n",
							"target_column_name = 'quantity'\n",
							"use_stores = [2, 5, 8,71,102]\n",
							"n_test_periods = 20\n",
							"\n",
							"\n",
							"#DataFrame\n",
							"df = data.toPandas()\n",
							"df[time_column_name] = pd.to_datetime(df[time_column_name])\n",
							"df['storeId'] = pd.to_numeric(df['storeId'])\n",
							"df['quantity'] = pd.to_numeric(df['quantity'])\n",
							"df['advertising'] = pd.to_numeric(df['advertising'])\n",
							"df['price'] = df['price'].astype(float)\n",
							"df['basePrice'] = df['basePrice'].astype(float)\n",
							"df['ratioAge60'] = df['ratioAge60'].astype(float)\n",
							"df['collegeRatio'] = df['collegeRatio'].astype(float)\n",
							"df['highIncome150Ratio'] = df['highIncome150Ratio'].astype(float)\n",
							"df['income'] = df['income'].astype(float)\n",
							"df['largeHH'] = df['largeHH'].astype(float)\n",
							"df['minoritiesRatio'] = df['minoritiesRatio'].astype(float)\n",
							"df['logQuantity'] = df['logQuantity'].astype(float)\n",
							"df['more1FullTimeEmployeeRatio'] = df['more1FullTimeEmployeeRatio'].astype(float)\n",
							"df['distanceNearestWarehouse'] = df['distanceNearestWarehouse'].astype(float)\n",
							"df['salesNearestWarehousesRatio'] = df['salesNearestWarehousesRatio'].astype(float)\n",
							"df['avgDistanceNearest5Supermarkets'] = df['avgDistanceNearest5Supermarkets'].astype(float)\n",
							"df['salesNearest5StoresRatio'] = df['salesNearest5StoresRatio'].astype(float)\n",
							"\n",
							"\n",
							"# Time Series\n",
							"data_subset = df[df.storeId.isin(use_stores)]\n",
							"nseries = data_subset.groupby(grain_column_names).ngroups\n",
							"print('Data subset contains {0} individual time-series.'.format(nseries))\n",
							"\n",
							"# Group by date\n",
							"def split_last_n_by_grain(df, n):\n",
							"    \"\"\"Group df by grain and split on last n rows for each group.\"\"\"\n",
							"    df_grouped = (df.sort_values(time_column_name) # Sort by ascending time\n",
							"                  .groupby(grain_column_names, group_keys=False))\n",
							"    df_head = df_grouped.apply(lambda dfg: dfg.iloc[:-n])\n",
							"    df_tail = df_grouped.apply(lambda dfg: dfg.iloc[-n:])\n",
							"    return df_head, df_tail\n",
							"\n",
							"# splitting\n",
							"train, test = split_last_n_by_grain(data_subset, n_test_periods)\n",
							"print(len(train),len(test))\n",
							"train.to_csv (r'./SurfaceSales_train.csv', index = None, header=True)\n",
							"test.to_csv (r'./SurfaceSales_test.csv', index = None, header=True)\n",
							"datastore = ws.get_default_datastore()\n",
							"datastore.upload_files(files = ['./SurfaceSales_train.csv', './SurfaceSales_test.csv'], target_path = 'dataset/', overwrite = True,show_progress = True)\n",
							"\n",
							"# loading the train dataset\n",
							"from azureml.core.dataset import Dataset\n",
							"train_dataset = Dataset.Tabular.from_delimited_files(path=datastore.path('dataset/SurfaceSales_train.csv'))"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Training the Models using AutoML Forecasting\n",
							"\n",
							"Please notice that **compute_target** is commented, meaning that the model training will run locally in Synapse Spark."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Parameters\n",
							"time_series_settings = {\n",
							"    'time_column_name': time_column_name,\n",
							"    'grain_column_names': grain_column_names,\n",
							"    'max_horizon': n_test_periods\n",
							"}\n",
							"\n",
							"# Config\n",
							"automl_config = AutoMLConfig(task='forecasting',\n",
							"                             debug_log='automl_ss_sales_errors.log',\n",
							"                             primary_metric='normalized_mean_absolute_error',\n",
							"                             experiment_timeout_hours=0.5,\n",
							"                             training_data=train_dataset,\n",
							"                             label_column_name=target_column_name,\n",
							"                             #compute_target=compute_target,\n",
							"                             enable_early_stopping=True,\n",
							"                             n_cross_validations=3,\n",
							"                             verbosity=logging.INFO,\n",
							"                             iterations=15,\n",
							"                             **time_series_settings)\n",
							"\n",
							"# Running the training\n",
							"remote_run = experiment.submit(automl_config, show_output=True)\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Retrieving the Best Model and Forecasting"
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Retrieving the best model\n",
							"best_run, fitted_model = remote_run.get_output()\n",
							"print(fitted_model.steps)\n",
							"model_name = best_run.properties['model_name']\n",
							"print(model_name)\n",
							"\n",
							"# Forecasting based on test dataset\n",
							"X_test = test\n",
							"y_test = X_test.pop(target_column_name).values\n",
							"X_test[time_column_name] = pd.to_datetime(X_test[time_column_name])\n",
							"y_predictions, X_trans = fitted_model.forecast(X_test)"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Plotting the Results"
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"import pandas as pd\n",
							"import numpy as np\n",
							"from pandas.tseries.frequencies import to_offset\n",
							"\n",
							"\n",
							"def align_outputs(y_predicted, X_trans, X_test, y_test, target_column_name,\n",
							"                  predicted_column_name='predicted',\n",
							"                  horizon_colname='horizon_origin'):\n",
							"    \"\"\"\n",
							"    Demonstrates how to get the output aligned to the inputs\n",
							"    using pandas indexes. Helps understand what happened if\n",
							"    the output's shape differs from the input shape, or if\n",
							"    the data got re-sorted by time and grain during forecasting.\n",
							"\n",
							"    Typical causes of misalignment are:\n",
							"    * we predicted some periods that were missing in actuals -> drop from eval\n",
							"    * model was asked to predict past max_horizon -> increase max horizon\n",
							"    * data at start of X_test was needed for lags -> provide previous periods\n",
							"    \"\"\"\n",
							"\n",
							"    if (horizon_colname in X_trans):\n",
							"        df_fcst = pd.DataFrame({predicted_column_name: y_predicted,\n",
							"                                horizon_colname: X_trans[horizon_colname]})\n",
							"    else:\n",
							"        df_fcst = pd.DataFrame({predicted_column_name: y_predicted})\n",
							"\n",
							"    # y and X outputs are aligned by forecast() function contract\n",
							"    df_fcst.index = X_trans.index\n",
							"\n",
							"    # align original X_test to y_test\n",
							"    X_test_full = X_test.copy()\n",
							"    X_test_full[target_column_name] = y_test\n",
							"\n",
							"    # X_test_full's index does not include origin, so reset for merge\n",
							"    df_fcst.reset_index(inplace=True)\n",
							"    X_test_full = X_test_full.reset_index().drop(columns='index')\n",
							"    together = df_fcst.merge(X_test_full, how='right')\n",
							"\n",
							"    # drop rows where prediction or actuals are nan\n",
							"    # happens because of missing actuals\n",
							"    # or at edges of time due to lags/rolling windows\n",
							"    clean = together[together[[target_column_name,\n",
							"                               predicted_column_name]].notnull().all(axis=1)]\n",
							"    return(clean)\n",
							"\n",
							"\n",
							"df_all = align_outputs(y_predictions, X_trans, X_test, y_test, target_column_name)\n",
							"\n",
							"#from azureml.automl.core._vendor.automl.client.core.common import metrics\n",
							"from matplotlib import pyplot as plt\n",
							"from automl.client.core.common import constants\n",
							"\n",
							"from azureml.automl.runtime.shared.score import scoring\n",
							"scores = scoring.score_regression(\n",
							"    y_test=df_all[target_column_name],\n",
							"    y_pred=df_all['predicted'],\n",
							"    metrics=list(constants.Metric.SCALAR_REGRESSION_SET))\n",
							"# use automl metrics module\n",
							"#scores = metrics.compute_metrics_regression(\n",
							" #   df_all['predicted'],\n",
							"  #  df_all[target_column_name],\n",
							"   # list(constants.Metric.SCALAR_REGRESSION_SET),\n",
							"    #None, None, None)\n",
							"\n",
							"print(\"[Test data scores]\\n\")\n",
							"for key, value in scores.items():    \n",
							"    print('{}:   {:.3f}'.format(key, value))\n",
							"    \n",
							"# Plot outputs\n",
							"#%matplotlib inline\n",
							"test_pred = plt.scatter(df_all[target_column_name], df_all['predicted'], color='b')\n",
							"test_test = plt.scatter(df_all[target_column_name], df_all[target_column_name], color='g')\n",
							"plt.legend((test_pred, test_test), ('prediction', 'truth'), loc='upper left', fontsize=8)\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"## Closing\n",
							"\n",
							"At this point you should have a chart like this one in the image below, created wit AutoML and MatplotLib. The results are that good because of the **logQuantity** column, a  data Leakage calculated from que **quantity** column. You can try to run the same experiment without it.\n",
							"\n",
							"<img src=\"https://synapse1poc.blob.core.windows.net/cosmolnksynp/cosmolnksynp/prediction.png\" alt=\"Chart\" width=\"75%\"/>\n",
							"\n",
							""
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/2-AnomalyDetectionWithMML')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "ws1sparkpool1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 19,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "19",
						"spark.dynamicAllocation.maxExecutors": "19"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/4eeedd72-d937-4243-86d1-c3982a84d924/resourceGroups/nashahz1007-02/providers/Microsoft.Synapse/workspaces/navzhtz6rgwnfydapocws1/bigDataPools/ws1sparkpool1",
						"name": "ws1sparkpool1",
						"type": "Spark",
						"endpoint": "https://navzhtz6rgwnfydapocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ws1sparkpool1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 5,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Leverage Power of Azure Synapse Link for Cosmos DB and Spark SQL\n",
							"With Synapse Link, you can now directly connect to your Azure Cosmos DB containers from Azure Synapse Analytics and access the analytical store with no separate connectors. This notebook scenario is to \n",
							"\n",
							"- Ingest streaming data into Azure Cosmos DB collection using Structured Streaming\n",
							"- Ingest Batch data into Azure Cosmos DB collection using Azure Synapse Spark\n",
							"- Format the stream dataframe as per the IoTSignals schema\n",
							"- Write the streaming dataframe to the Azure Cosmos DB collection\n",
							"- Perform Joins and aggregations across Azure Cosmos DB collections using Azure Synapse Link\n",
							"- Perform Anomaly Detection using Azure Synapse Link and Azure Cognitive Services on Synapse Spark (MMLSpark)"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<img src=\"https://synapse1poc.blob.core.windows.net/cosmolnksynp/cosmolnksynp/synapse-cosmosdb.png\" alt=\"Surface Device\" width=\"75%\"/>\r\n",
							"\r\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Replace the Cognitive Services API account Keys below before execution.Keys can be found under \"Keys and Endpoints\" Section\r\n",
							"<img src=\"https://synapse1poc.blob.core.windows.net/cosmolnksynp/cosmolnksynp/AD_Ws_Varriables.gif\" alt=\"Surface Device\" width=\"75%\"/>\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Cognitive_Services_Key='<Cognitive Services Account Key>'"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Simulate Streaming Data Generation using Rate Streaming Source\r\n",
							"The Rate streaming source is used to simplify the solution here and can be replaced with any supported streaming sources such as Azure Event Hubs and Apache Kafka."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"dfStream = (spark\r\n",
							"                .readStream\r\n",
							"                .format(\"rate\")\r\n",
							"                .option(\"rowsPerSecond\", 10)\r\n",
							"                .load()\r\n",
							"            )"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Format the Stream Dataframe as per the IoTSignals Schema"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"import pyspark.sql.functions as F\n",
							"from pyspark.sql.types import StringType\n",
							"import uuid\n",
							"\n",
							"numberOfDevices = 10\n",
							"generate_uuid = F.udf(lambda : str(uuid.uuid4()), StringType())\n",
							"              \n",
							"dfIoTSignals = (dfStream\n",
							"                    .withColumn(\"id\", generate_uuid())\n",
							"                    .withColumn(\"deviceId\", F.concat(F.lit(\"dev-\"), F.expr(\"mod(value, %d)\" % numberOfDevices)))\n",
							"                    .withColumn(\"dateTime\", dfStream[\"timestamp\"].cast(StringType()))\n",
							"                    .withColumn(\"unit\", F.expr(\"CASE WHEN rand() < 0.5 THEN 'Revolutions per Minute' ELSE 'MegaWatts' END\"))\n",
							"                    .withColumn(\"unitSymbol\", F.expr(\"CASE WHEN rand() < 0.5 THEN 'RPM' ELSE 'MW' END\"))\n",
							"                    .withColumn(\"measureType\", F.expr(\"CASE WHEN rand() < 0.5 THEN 'Rotation Speed' ELSE 'Output' END\"))\n",
							"                    .withColumn(\"measureValue\", F.expr(\"CASE WHEN rand() > 0.95 THEN value * 10 WHEN rand() < 0.05 THEN value div 10 ELSE value END\"))\n",
							"                    .drop(\"timestamp\", \"value\")\n",
							"                )"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Stream Writes to the Azure Cosmos DB Collection\r\n",
							"The \"cosmos.oltp\" is the Spark format that enables connection to the Cosmos DB Transactional store."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"streamQuery = dfIoTSignals\\\r\n",
							"                    .writeStream\\\r\n",
							"                    .format(\"cosmos.oltp\")\\\r\n",
							"                    .outputMode(\"append\")\\\r\n",
							"                    .option(\"spark.cosmos.connection.mode\", \"gateway\") \\\r\n",
							"                    .option(\"spark.synapse.linkedService\", \"CosmosDBLink\")\\\r\n",
							"                    .option(\"spark.cosmos.container\", \"IoTSignals\")\\\r\n",
							"                    .option(\"checkpointLocation\", \"/writeCheckpointDir\")\\\r\n",
							"                    .start()\r\n",
							"streamQuery.awaitTermination(180)\r\n",
							"streamQuery.stop()"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Load the IoTDeviceInfo Dataset from ADLS Gen2 to a Dataframe and Write the Dataframe to the Azure Cosmos DB Collection\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"path = 'wasbs://cosmolnksynp@synapse1poc.blob.core.windows.net/cosmolnksynp/IoTDeviceInfo.csv'\r\n",
							"dfDeviceInfo = spark.read.csv(path, header=True, inferSchema=True)\r\n",
							"dfDeviceInfo.write\\\r\n",
							"    .format(\"cosmos.oltp\")\\\r\n",
							"    .option(\"spark.synapse.linkedService\", \"CosmosDBLink\")\\\r\n",
							"    .option(\"spark.cosmos.container\", \"IoTDeviceInfo\")\\\r\n",
							"    .option(\"spark.cosmos.write.upsertEnabled\", \"true\")\\\r\n",
							"    .mode('append')\\\r\n",
							"    .save()"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create Spark Tables Pointing to the Azure Cosmos DB Analytical Store Collections using Azure Synapse Link\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"create database if not exists CosmosDBIoTDemoDB;\r\n",
							"\r\n",
							"create table if not exists CosmosDBIoTDemoDB.IoTSignals\r\n",
							"using cosmos.olap\r\n",
							"options(spark.synapse.linkedService 'CosmosDBLink',\r\n",
							"        spark.cosmos.container 'IoTSignals');\r\n",
							"\r\n",
							"create table if not exists CosmosDBIoTDemoDB.IoTDeviceInfo\r\n",
							"using cosmos.olap\r\n",
							"options(spark.synapse.linkedService 'CosmosDBLink',\r\n",
							"        spark.cosmos.container 'IoTDeviceInfo')\r\n",
							""
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Perform Joins Across collections, Apply Filters and Aggregations using Spark SQL"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df_RPM_details = spark.sql(\"select a.deviceid \\\r\n",
							"                                 , b.devicetype \\\r\n",
							"                                 , cast(b.location as string) as location\\\r\n",
							"                                 , cast(b.latitude as float) as latitude\\\r\n",
							"                                 , cast(b.longitude as float) as  longitude\\\r\n",
							"                                 , a.measuretype \\\r\n",
							"                                 , a.unitSymbol \\\r\n",
							"                                 , cast(sum(measureValue) as float) as measureValueSum \\\r\n",
							"                                 , count(*) as count \\\r\n",
							"                            from CosmosDBIoTDemoDB.IoTSignals a \\\r\n",
							"                            left join CosmosDBIoTDemoDB.IoTDeviceInfo b \\\r\n",
							"                            on a.deviceid = b.deviceid \\\r\n",
							"                            where a.unitSymbol = 'RPM' \\\r\n",
							"                            group by a.deviceid, b.devicetype, b.location, b.latitude, b.longitude, a.measuretype, a.unitSymbol\")\r\n",
							"\r\n",
							"display(df_RPM_details)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Visualizations using Plotly and DisplayHTML()\r\n",
							"Heatmap of IoT signals across diffrent locations\r\n",
							"\r\n",
							"<img src=\"https://synapse1poc.blob.core.windows.net/cosmolnksynp/cosmolnksynp/HeatMaPlot.png\" alt=\"Chart\" width=\"75%\"/>\r\n",
							"\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from plotly.offline import plot\r\n",
							"import plotly.express as px\r\n",
							"\r\n",
							"df_RPM_details_pd = df_RPM_details.toPandas()\r\n",
							"fig = px.scatter_mapbox(df_RPM_details_pd, \r\n",
							"                        lat='latitude', \r\n",
							"                        lon='longitude', \r\n",
							"                        size = 'measureValueSum',\r\n",
							"                        color = 'measureValueSum',\r\n",
							"                        hover_name = 'location',\r\n",
							"                        hover_data = ['measureValueSum','location'],\r\n",
							"                        size_max = 30,\r\n",
							"                        color_continuous_scale = px.colors.carto.Temps,\r\n",
							"                        zoom=3,\r\n",
							"                        height=600,\r\n",
							"                        width =900)\r\n",
							"\r\n",
							"fig.update_layout(mapbox_style='open-street-map')\r\n",
							"fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\r\n",
							"\r\n",
							"p = plot(fig,output_type='div')\r\n",
							"displayHTML(p)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Load the Data in Cosmos DB Analytical Store Collection into a Dataframe using Synapse Link"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"df_IoTSignals = spark.read\\\r\n",
							"                    .format(\"cosmos.olap\")\\\r\n",
							"                    .option(\"spark.synapse.linkedService\", \"CosmosDBLink\")\\\r\n",
							"                    .option(\"spark.cosmos.container\", \"IoTSignals\")\\\r\n",
							"                    .load()"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Data Exploration using Pyplot\r\n",
							"\r\n",
							"\r\n",
							"<img src=\"https://synapse1poc.blob.core.windows.net/cosmolnksynp/cosmolnksynp/DataExPlot.png\" alt=\"Chart\" width=\"75%\"/>"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas as pd\r\n",
							"import matplotlib.pyplot as plt\r\n",
							"\r\n",
							"df_IoTSignals_pd = df_IoTSignals.toPandas().dropna()\r\n",
							"df_IoTSignals_pd['measureValue'] = df_IoTSignals_pd['measureValue'].astype(int)\r\n",
							"\r\n",
							"df_MW = df_IoTSignals_pd[(df_IoTSignals_pd['deviceId'] == 'dev-1') & (df_IoTSignals_pd['unitSymbol'] == 'MW')]\r\n",
							"df_MW.plot(x='dateTime', y='measureValue', color='green', figsize=(20,5), label = 'Output MW')\r\n",
							"plt.title('MW TimeSeries')\r\n",
							"plt.show()\r\n",
							"\r\n",
							"df_RPM = df_IoTSignals_pd[(df_IoTSignals_pd['deviceId'] == 'dev-1') & (df_IoTSignals_pd['unitSymbol'] == 'RPM')]\r\n",
							"df_RPM.plot(x='dateTime', y='measureValue', color='black', figsize=(20,5), label = 'Output RPM')\r\n",
							"plt.title('RPM TimeSeries')\r\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Perform Anomaly Detection using Microsoft Machine Learning for Spark (MMLSpark)"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql.functions import col\n",
							"from pyspark.sql.types import *\n",
							"from mmlspark.cognitive import SimpleDetectAnomalies\n",
							"from mmlspark.core.spark import FluentAPI\n",
							"\n",
							"anomaly_detector = (SimpleDetectAnomalies()\n",
							"                            .setSubscriptionKey(Cognitive_Services_Key)\n",
							"                            .setLocation('southcentralus') ##Optional, Update If Required\n",
							"                            .setOutputCol(\"anomalies\")\n",
							"                            .setGroupbyCol(\"grouping\")\n",
							"                            .setSensitivity(95)\n",
							"                            .setGranularity(\"secondly\"))\n",
							"\n",
							"df_anomaly = (df_IoTSignals\n",
							"                    .where(col(\"unitSymbol\") == 'RPM')\n",
							"                    .withColumnRenamed(\"dateTime\", \"timestamp\")\n",
							"                    .withColumn(\"value\", col(\"measureValue\").cast(\"double\"))\n",
							"                    .withColumn(\"grouping\", col(\"deviceId\"))\n",
							"                    .mlTransform(anomaly_detector))\n",
							"\n",
							"df_anomaly.createOrReplaceTempView('df_anomaly')"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(df_anomaly)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Format the Dataframe for Visualization\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df_anomaly_single_device = spark.sql(\"select timestamp \\\r\n",
							"                                            , measureValue \\\r\n",
							"                                            , anomalies.expectedValue \\\r\n",
							"                                            , anomalies.expectedValue + anomalies.upperMargin as expectedUpperValue \\\r\n",
							"                                            , anomalies.expectedValue - anomalies.lowerMargin as expectedLowerValue \\\r\n",
							"                                            , case when anomalies.isAnomaly=true then 1 else 0 end as isAnomaly \\\r\n",
							"                                        from df_anomaly \\\r\n",
							"                                        where deviceid = 'dev-1' \\\r\n",
							"                                        order by timestamp \\\r\n",
							"                                        limit 400\")\r\n",
							"\r\n",
							"display(df_anomaly_single_device)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Visualize the Anomalies using Plotly\r\n",
							"Plot Expected value, Upper Value, Lower Value and Actual Value along with Anomaly flag\r\n",
							"\r\n",
							"<img src=\"https://synapse1poc.blob.core.windows.net/cosmolnksynp/cosmolnksynp/AnmaliesPlot.png\" alt=\"Chart\" width=\"75%\"/>\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"##import chart_studio.plotly as py\r\n",
							"import plotly.graph_objs as go\r\n",
							"from plotly.offline import plot\r\n",
							"import matplotlib.pyplot as plt\r\n",
							"from pyspark.sql.functions import col\r\n",
							"from matplotlib.pyplot import figure\r\n",
							" \r\n",
							"adf = df_anomaly_single_device.toPandas()\r\n",
							"adf_subset = df_anomaly_single_device.where(col(\"isAnomaly\") == 1).toPandas() \r\n",
							"\r\n",
							"plt.figure(figsize=(23,8))\r\n",
							"plt.plot(adf['timestamp'],adf['expectedUpperValue'], color='darkred', linestyle='solid', linewidth=0.25)\r\n",
							"plt.plot(adf['timestamp'],adf['expectedValue'], color='darkgreen', linestyle='solid', linewidth=2)\r\n",
							"plt.plot(adf['timestamp'],adf['measureValue'], 'b', color='royalblue', linestyle='dotted', linewidth=2)\r\n",
							"plt.plot(adf['timestamp'],adf['expectedLowerValue'],  color='black', linestyle='solid', linewidth=0.25)\r\n",
							"plt.plot(adf_subset['timestamp'],adf_subset['measureValue'], 'ro')\r\n",
							"plt.legend(['RPM-UpperMargin', 'RPM-ExpectedValue', 'RPM-ActualValue', 'RPM-LowerMargin', 'RPM-Anomaly'])\r\n",
							"plt.title('RPM Anomalies with Expected, Actual, Upper and Lower Values')\r\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 16
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ws1sparkpool1')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 40,
					"minNodeCount": 3
				},
				"nodeCount": 5,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "2.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/vsayxg7ydw3tzkjqpocws1p1')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks",
			"apiVersion": "2019-06-01-preview",
			"properties": {},
			"dependsOn": []
		}
	]
}